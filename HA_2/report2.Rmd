---
title: "Home Assignment 2"
author: "Alfons"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Task 4
The total variation, or all of the information, is given by the sum of all eigenvalues. The function cutoff85 below uses the formula:
$\frac{\sum_{i=1}^{q}\lambda_i}{\sum_{i=1}^{p}\lambda_i} = 85\%$
to calculate the number of principal components required for keeping 85% of the total information (or variation).
```{r}
cutoff85 <- function(){
  tot_eigv = sum(eigen_dec[["values"]])
  i = 1
  cond = 0
  while(cond <= 0.85){
    eigv = sum(eigen_dec[["values"]][1:i])
    cond = eigv/tot_eigv
    i = i + 1
  }
  return(i-1)
}
```
The cutoff85 function stops once the cond is equal or bigger than 0.85 and returns the number of principal components used.
```{r}
num_pcs <- cutoff85()
num_pcs # number of pcs required for keeping  85% the information
```
To keep 85% of the total information we only need 33 principal components.

## Task 5
In this task we will build a Gaussian mixture model using the first two principal components. This GMM will then be used to classify images as a five or six. First, all the images of fives and sixes need to be separated from the rest, then the all the images of fives and sixes need to be separated into a training set and testing set. The eigendecomposition is done the on covariance matrix from the training set, the eigenvectors from the decompistion will then be used to calculate the first two principal components for both the training and testing set.

```{r}
# Task 5
Five_Six <- data[which(data[,1]==5 | data[,1] == 6),2:257]
Five_Six_w_labels <- data[which(data[,1]==5 | data[,1] == 6),1:257]

n = dim(Five_Six)[1] # sample size
set.seed(2022)
id = sample(1:n, floor(0.8*n)) 
train_data = Five_Six[id, ] # training set
test_data = Five_Six[-id, ] # testing set


S = cov(train_data) # training covariance matrix
mu = colMeans(train_data) # mean vector

# eigen decomposition of covariance matrix
eigen_dec = eigen(S) 
eigen_val = eigen_dec$values
eigen_vec = eigen_dec$vectors
```

The first two principal components from the decomposition:
```{r}
# Get the first 2 principal components for train data
PC1 = numeric(length(train_data[, 1]))
PC2 = numeric(length(train_data[, 1]))
```

Calculate the first two principal components for all images in the training set:
```{r}
for (i in 1:length(train_data[, 1])){
  z1 = t(train_data[i, ] - mu) %*% eigen_vec[, 1]
  z2 = t(train_data[i, ] - mu) %*% eigen_vec[, 2]
  
  PC1[i] = z1
  PC2[i] = z2
}
```

We then create a new dataframe containing only the labels as well as the principal components:
```{r}
# create new dataframe with labels and principal components for train data
train_labels = Five_Six_w_labels[id, 1]
test_dat = data.frame(train_labels, PC1, PC2)
```

This new dataframe is separated further into two dataframes: train_fives and train_sixs:
```{r}
# separate 5s and 6s
train_fives = test_dat[which(test_dat[,1]==5),2:3]
train_sixs = test_dat[which(test_dat[,1]==6),2:3]

```

This is done in order to calculate the covariance matrix and mean vectors for both groups (5s and 6s), these will then be used in the GMM classifier for prediction.
```{r}
# Get statistics for 5s and 6s
mu5 = colMeans(train_fives)
S5 = cov(train_fives)

mu6 = colMeans(train_sixs)
S6 = cov(train_sixs)
```

The principal components for the testing data is handled the same way as the training data, using the same eigenvectors being the important part.
```{r}
# Get test data

test_mu = colMeans(test_data)

test_PC1 = numeric(length(test_data[, 1]))
test_PC2 = numeric(length(test_data[, 1]))

for (i in 1:length(test_data[, 1])) {
  z1 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 1]
  z2 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 2]
  
  test_PC1[i] = z1
  test_PC2[i] = z2
}
```

We then build a new dataframe (like we did for the training data), only consisting of labels and the principal components.
```{r}
# create new dataframe with labels and principal components for test data
test_labels = Five_Six_w_labels[-id, 1]
test_dat = data.frame(test_labels, test_PC1, test_PC2)
```

The new dataframes are done, classification soon be done. The following functions are repurposed from HA1, with slight modifications.
```{r}
# classifier functions
exponent = function(my_vector, row_observation, cov_matrix){
  x = as.vector(row_observation) - as.vector(my_vector)
  result = exp(-t(t(x)) %*% solve(cov_matrix) %*% t(x)/2)
  return(result)
}

# The base/quota of the multinomial distribution
base = function(cov_matrix, p){
  result <- 1/(((2*pi)^(p/2))*sqrt(det(cov_matrix)))
  return(result)
}

# classify an observation as malignant=1, benign = 0
classifier <- function(row_observation, five_prob, six_prob){
  five <- base(S5, length(row_observation)) * exponent(mu5, row_observation, S5) * five_prob
  six <- base(S6, length(row_observation)) * exponent(mu6, row_observation, S6) * six_prob
  if (six > five){
    prediction <- 6
  } else{
    prediction <- 5
  }
  return(prediction)
}
```

If the image is predicted to be a five, the numbers five is returned, if the image is predicted to be a six, a six is returned. Before we can start classifying we need the initial probabilities of getting a five and six, respectively.
```{r}
five_prob = length(train_fives[,1]) / length(train_data[, 1]) 
six_prob = length(train_sixs[,1]) / length(train_data[, 1])
```

The classifying is done in the for-loop below.
```{r}
num_five = 0
num_six = 0
pred_col <- c()
for (i in 1:nrow(test_dat)) {
  guess <- classifier(test_dat[i, 2:3], five_prob, six_prob) # return 5 or 6
  if(guess == 5){ # guess is 5=five
    num_five = num_five + 1
    pred_col <- append(pred_col, 5)
  }else{ # guess is 6=six
    num_six = num_six + 1
    pred_col <- append(pred_col, 6)
  }
}
```

Print the number of fives and sixes.
```{r}
# Print number of malignant and benign in test data
num_five
num_six
```

The accuracy of the classifier is calculated by going through the test data and see if the label (a 5 or 6) matches the number given by the classifier.
```{r}
# Calculate the classifier accuracy
cor_pred <- 0
wr_pred <- 0
for (i in 1:length(pred_col)){
  if(pred_col[i]==5 & test_dat[i,1]==5){ # if predict = 1 and diagnosis = "M"
    cor_pred <- cor_pred + 1
  }
  if(pred_col[i]==6 & test_data[i,1]==6){ # if predict = 0 and diagnosis = "B"
    cor_pred <- cor_pred + 1
  }
}
```

The accuracy of the classifier:
```{r}
accuracy = cor_pred / length(pred_col)
accuracy


cor_pred
wr_pred
```

