---
title: "Home Assignment 2"
author: "Alfons"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Task 4
The total variation, or all of the information, is given by the sum of all eigenvalues. The function cutoff85 below uses the formula:
$\frac{\sum_{i=1}^{q}\lambda_i}{\sum_{i=1}^{p}\lambda_i} = 85\%$
to calculate the number of principal components required for keeping 85% of the total information (or variation).
```{r}
cutoff85 <- function(){
  tot_eigv = sum(eigen_dec[["values"]])
  i = 1
  cond = 0
  while(cond <= 0.85){
    eigv = sum(eigen_dec[["values"]][1:i])
    cond = eigv/tot_eigv
    i = i + 1
  }
  return(i-1)
}
```
The cutoff85 function stops once the cond is equal or bigger than 0.85 and returns the number of principal components used.
```{r}
num_pcs <- cutoff85()
num_pcs # number of pcs required for keeping  85% the information
```
To keep 85% of the total information we only need 33 principal components.

## Task 5
In this task we will build a Gaussian mixture model using the first two principal components. This GMM will then be used to classify images as a five or six. First, all the images of fives and sixes need to be separated from the rest, then the all the images of fives and sixes need to be separated into a training set and testing set. The eigendecomposition is done the on covariance matrix from the training set, the eigenvectors from the decompistion will then be used to calculate the first two principal components for both the training and testing set.

```{r}
# Task 5
Five_Six <- data[which(data[,1]==5 | data[,1] == 6),2:257]
Five_Six_w_labels <- data[which(data[,1]==5 | data[,1] == 6),1:257]

n = dim(Five_Six)[1] # sample size
set.seed(2022)
id = sample(1:n, floor(0.8*n)) 
train_data = Five_Six[id, ] # training set
test_data = Five_Six[-id, ] # testing set


S = cov(train_data) # training covariance matrix
mu = colMeans(train_data) # mean vector

# eigen decomposition of covariance matrix
eigen_dec = eigen(S) 
eigen_val = eigen_dec$values
eigen_vec = eigen_dec$vectors
```

The first two principal components from the decomposition:
```{r}
# Get the first 2 principal components for train data
PC1 = numeric(length(train_data[, 1]))
PC2 = numeric(length(train_data[, 1]))
```

Calculate the first two principal components for the training set:
```{r}
for (i in 1:length(train_data[, 1])){
  z1 = t(train_data[i, ] - mu) %*% eigen_vec[, 1]
  z2 = t(train_data[i, ] - mu) %*% eigen_vec[, 2]
  
  PC1[i] = z1
  PC2[i] = z2
}
```

We then create a new data frame containing only the labels as well as the principal components:
```{r}
# create new dataframe with labels and principal components for train data
train_labels = Five_Six_w_labels[id, 1]
test_dat = data.frame(train_labels, PC1, PC2)
```

This new dataframe is separated further into two dataframes: train_fives and train_sixs:
```{r}
# separate 5s and 6s
train_fives = test_dat[which(test_dat[,1]==5),2:3]
train_sixs = test_dat[which(test_dat[,1]==6),2:3]

```

This is done in order to calculate the covariance matrix and mean vectors for both groups, these will then be used in the GMM classifier for prediction.
```{r}
# Get statistics for 5s and 6s
mu5 = colMeans(train_fives)
S5 = cov(train_fives)

mu6 = colMeans(train_sixs)
S6 = cov(train_sixs)
```

The principal components for the testing data is handled the same way as the training data, using the same eigenvectors being the important part.
```{r}
# Get test data

test_mu = colMeans(test_data)

test_PC1 = numeric(length(test_data[, 1]))
test_PC2 = numeric(length(test_data[, 1]))

for (i in 1:length(test_data[, 1])) {
  z1 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 1]
  z2 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 2]
  
  test_PC1[i] = z1
  test_PC2[i] = z2
}
```

we then build a new dataframe (like we did for the training data), only consisting of labels and the principal components.
```{r}
# create new dataframe with labels and principal components for test data
test_labels = Five_Six_w_labels[-id, 1]
test_dat = data.frame(test_labels, test_PC1, test_PC2)
```

